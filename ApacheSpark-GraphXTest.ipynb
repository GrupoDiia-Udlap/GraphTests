{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating graph metrics with GraphX\n",
    "\n",
    "The purpouse of this notebook is to use the distributed graph processing engine called [GraphX](http://graphstream-project.org) for graph handling.\n",
    "\n",
    "Test main features:\n",
    "1. The test was implemented in [Scala 2.11](https://www.scala-lang.org/download/2.11.12.html)\n",
    "2. The processing is handled by [Spark](https://github.com/apache/spark) a cluster computing framework.\n",
    "3. The libraries required to run the test are the following:    \n",
    "    * [Cassandra connector](https://github.com/datastax/spark-cassandra-connector) (2.11 scala build version).\n",
    "    * [Spark](https://mvnrepository.com/artifact/org.apache.spark/spark-core_2.11/2.2.1) (2.11 scala build version).\n",
    "    * [Spark GraphX](http://) (2.11 scala build version).\n",
    "\n",
    "4. The Spark version used correspond to standalone application which mean that the use of multiple hosts ecosystem to test isn't needed.\n",
    "\n",
    "# Step 1: Load libraries from Maven\n",
    "In order to download the required libraries from the Maven repositories we need to use the following instructions (special Jupyter notebook's commands that allow Maven integration). In a traditional development environment we use a POM (Maven) or build.sbt (SBT) file to define the dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 114 artifact(s)\n",
      "Adding 13 artifact(s)\n",
      "Adding 11 artifact(s)\n",
      "Adding 4 artifact(s)\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "//Import dependencies from Maven\n",
    "classpath.add(\"org.apache.spark\" % \"spark-core_2.11\" % \"2.1.1\")\n",
    "classpath.add(\"org.apache.spark\" % \"spark-sql_2.11\" % \"2.1.1\")\n",
    "classpath.add(\"org.apache.spark\" % \"spark-graphx_2.11\" % \"2.1.1\")\n",
    "classpath.add(\"com.datastax.spark\" % \"spark-cassandra-connector_2.11\" % \"2.0.0-M3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Create a Spark context\n",
    "\n",
    "Before start any computation we need to create a Spark context.\n",
    "\n",
    "The following code set up Spark's configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36morg.apache.spark.SparkConf\u001b[0m\n",
       "\u001b[36mconfiguration\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mapache\u001b[0m.\u001b[32mspark\u001b[0m.\u001b[32mSparkConf\u001b[0m = org.apache.spark.SparkConf@1c6fd88"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// Set up Spark's configuration\n",
    "  import org.apache.spark.SparkConf\n",
    "  val configuration = new SparkConf()\n",
    "    .setAppName(\"GraphXTest\")\n",
    "    .setMaster(\"local[*]\")\n",
    "    .set(\"spark.executor.memory\", \"1g\")\n",
    "    .set(\"spark.testing.memory\", \"2147480000\")// Avoid any memory issues\n",
    "    .set(\"spark.cassandra.connection.host\", \"127.0.0.1\")\n",
    "    .set(\"spark.cassandra.auth.username\", \"cassandra\")\n",
    "    .set(\"spark.cassandra.auth.password\", \"cassandra\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once configured, the Spark context can be created.\n",
    "\n",
    "The following code initialize the Spark context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "18/11/13 10:11:35 INFO SparkContext: Running Spark version 2.1.1\n",
      "18/11/13 10:11:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "18/11/13 10:11:39 WARN Utils: Your hostname, spark resolves to a loopback address: 127.0.1.1; using 192.168.1.207 instead (on interface eth0)\n",
      "18/11/13 10:11:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "18/11/13 10:11:39 INFO SecurityManager: Changing view acls to: jose\n",
      "18/11/13 10:11:39 INFO SecurityManager: Changing modify acls to: jose\n",
      "18/11/13 10:11:39 INFO SecurityManager: Changing view acls groups to: \n",
      "18/11/13 10:11:39 INFO SecurityManager: Changing modify acls groups to: \n",
      "18/11/13 10:11:39 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(jose); groups with view permissions: Set(); users  with modify permissions: Set(jose); groups with modify permissions: Set()\n",
      "18/11/13 10:11:44 INFO Utils: Successfully started service 'sparkDriver' on port 39826.\n",
      "18/11/13 10:11:45 INFO SparkEnv: Registering MapOutputTracker\n",
      "18/11/13 10:11:45 INFO SparkEnv: Registering BlockManagerMaster\n",
      "18/11/13 10:11:45 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "18/11/13 10:11:45 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "18/11/13 10:11:45 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-9ea33c23-f877-4e53-aa45-e3bfd915bfa2\n",
      "18/11/13 10:11:45 INFO MemoryStore: MemoryStore started with capacity 1048.8 MB\n",
      "18/11/13 10:11:46 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "18/11/13 10:11:48 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "18/11/13 10:11:48 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.207:4040\n",
      "18/11/13 10:11:48 INFO Executor: Starting executor ID driver on host localhost\n",
      "18/11/13 10:11:49 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36503.\n",
      "18/11/13 10:11:49 INFO NettyBlockTransferService: Server created on 192.168.1.207:36503\n",
      "18/11/13 10:11:49 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "18/11/13 10:11:49 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.207, 36503, None)\n",
      "18/11/13 10:11:49 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.207:36503 with 1048.8 MB RAM, BlockManagerId(driver, 192.168.1.207, 36503, None)\n",
      "18/11/13 10:11:49 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.207, 36503, None)\n",
      "18/11/13 10:11:49 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.1.207, 36503, None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36morg.apache.spark.SparkContext\u001b[0m\n",
       "\u001b[36msc\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mapache\u001b[0m.\u001b[32mspark\u001b[0m.\u001b[32mSparkContext\u001b[0m = org.apache.spark.SparkContext@964a3e"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// Initialize Spark context from configuration\n",
    "import org.apache.spark.SparkContext\n",
    "val sc = new SparkContext(configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Retrieve interactions from Cassandra\n",
    "To populate the graph we retrieve the course interactions from Cassandra.\n",
    "\n",
    "First we specify interaction's datatype:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass \u001b[36mInteraccion\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// Interaccion datatype\n",
    "case class Interaccion(idinteraccion: String, atributos: Map[String, String])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next lines of code read interactions from Cassandra and save into RDD (Resilient Distributed Dataset):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36mcom.datastax.spark.connector._\u001b[0m\n",
       "\u001b[36mcourseId\u001b[0m: \u001b[32mString\u001b[0m = \u001b[32m\"1\"\u001b[0m\n",
       "\u001b[36minteractions\u001b[0m: \u001b[32mArray\u001b[0m[\u001b[32mInteraccion\u001b[0m] = \u001b[33mArray\u001b[0m(\n",
       "  \u001b[33mInteraccion\u001b[0m(\n",
       "    \u001b[32m\"040be443-3492-4757-8710-beef16401100\"\u001b[0m,\n",
       "    \u001b[33mMap\u001b[0m(\n",
       "      \u001b[32m\"tipo_interaccion\"\u001b[0m -> \u001b[32m\"vis\"\u001b[0m,\n",
       "      \u001b[32m\"timestamp\"\u001b[0m -> \u001b[32m\"2010-10-14 05:04:51.482\"\u001b[0m,\n",
       "      \u001b[32m\"tipo_contenido\"\u001b[0m -> \u001b[32m\"des\"\u001b[0m,\n",
       "      \u001b[32m\"id_curso_origen\"\u001b[0m -> \u001b[32m\"1\"\u001b[0m,\n",
       "      \u001b[32m\"nodo_destino\"\u001b[0m -> \u001b[32m\"\"\u001b[0m,\n",
       "      \u001b[32m\"contenido\"\u001b[0m -> \u001b[32m\"\"\u001b[0m,\n",
       "      \u001b[32m\"plataforma\"\u001b[0m -> \u001b[32m\"p\"\u001b[0m,\n",
       "      \u001b[32m\"id_curso_destino\"\u001b[0m -> \u001b[32m\"1\"\u001b[0m,\n",
       "      \u001b[32m\"sentimiento\"\u001b[0m -> \u001b[32m\"\"\u001b[0m,\n",
       "      \u001b[32m\"nodo_origen\"\u001b[0m -> \u001b[32m\"17\"\u001b[0m\n",
       "    )\n",
       "  ),\n",
       "  \u001b[33mInteraccion\u001b[0m(\n",
       "    \u001b[32m\"db1c65e6-7436-4398-8404-a6d60facccd2\"\u001b[0m,\n",
       "    \u001b[33mMap\u001b[0m(\n",
       "      \u001b[32m\"tipo_interaccion\"\u001b[0m -> \u001b[32m\"pub\"\u001b[0m,\n",
       "\u001b[33m...\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// Read interacciones from Cassandra database\n",
    "import com.datastax.spark.connector._ //Loads implicit functions\n",
    "val courseId = \"1\"\n",
    "val interactions = sc.cassandraTable[Interaccion](\"diia\", \"interacciones\")\n",
    "    .where(\"atributos['id_curso_origen']=? AND atributos['nodo_destino']=''\", courseId)\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Build a graph from interactions\n",
    "\n",
    "The next code iterate over each interaction previously fetched to fill the graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph's node count:24"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36morg.apache.spark.graphx\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.graphx.Graph\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.rdd.RDD\u001b[0m\n",
       "\u001b[36medges\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mapache\u001b[0m.\u001b[32mspark\u001b[0m.\u001b[32mrdd\u001b[0m.\u001b[32mRDD\u001b[0m[\u001b[32morg\u001b[0m.\u001b[32mapache\u001b[0m.\u001b[32mspark\u001b[0m.\u001b[32mgraphx\u001b[0m.\u001b[32mEdge\u001b[0m[\u001b[32mInteraccion\u001b[0m]] = ParallelCollectionRDD[2] at parallelize at Main.scala:39\n",
       "\u001b[36mgraphXGraph\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mapache\u001b[0m.\u001b[32mspark\u001b[0m.\u001b[32mgraphx\u001b[0m.\u001b[32mGraph\u001b[0m[\u001b[32mDouble\u001b[0m, \u001b[32mInteraccion\u001b[0m] = org.apache.spark.graphx.impl.GraphImpl@118ad9f"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.graphx\n",
    "import org.apache.spark.graphx.Graph\n",
    "import org.apache.spark.rdd.RDD\n",
    "\n",
    "val edges: RDD[graphx.Edge[Interaccion]] = sc.parallelize(interactions.map(\n",
    "interaccion => {\n",
    "  graphx.Edge(interaccion.atributos.get(\"nodo_origen\").get.toLong, interaccion.atributos.get(\"nodo_destino\").get match {\n",
    "    case \"\" => 0\n",
    "    case s:String => s.toLong\n",
    "  }, interaccion)\n",
    "\n",
    "}\n",
    ").toSeq)\n",
    "\n",
    "// Set edges into new graph\n",
    "val graphXGraph = Graph.fromEdges(edges,0.0)\n",
    "print(\"Graph's node count:\" + graphXGraph.vertices.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Calculate graphx's algorithms\n",
    "\n",
    "In this section we will calculate the graphx's build-in algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Course (graph id,org.apache.spark.graphx.impl.GraphImpl@9e9d28):\n",
      " Node count: 24 \n",
      " Triangle count org.apache.spark.graphx.impl.GraphImpl@e22aea.\n",
      "-->Nodo 0 -> Pagerank: 4.449999077359371\n",
      "-->Nodo 13 -> Pagerank: 0.8500000000000001\n",
      "-->Nodo 19 -> Pagerank: 0.8500000000000001\n",
      "-->Nodo 15 -> Pagerank: 0.8500000000000001\n",
      "-->Nodo 4 -> Pagerank: 0.8500000000000001\n",
      "-->Nodo 21 -> Pagerank: 0.8500000000000001\n",
      "-->Nodo 16 -> Pagerank: 0.8500000000000001\n",
      "-->Nodo 22 -> Pagerank: 0.8500000000000001\n",
      "-->Nodo 25 -> Pagerank: 0.8500000000000001\n",
      "-->Nodo 11 -> Pagerank: 0.8500000000000001\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36morg.apache.spark.graphx._\u001b[0m\n",
       "\u001b[36mpr\u001b[0m: \u001b[32mgraphx\u001b[0m.\u001b[32mVertexRDD\u001b[0m[\u001b[32mDouble\u001b[0m] = VertexRDDImpl[185] at RDD at VertexRDD.scala:57\n",
       "\u001b[36mcomputedGraphx\u001b[0m: (\u001b[32mString\u001b[0m, \u001b[32mGraph\u001b[0m[\u001b[32mDouble\u001b[0m, \u001b[32mInteraccion\u001b[0m]) = \u001b[33m\u001b[0m(\u001b[32m\"graph id\"\u001b[0m, org.apache.spark.graphx.impl.GraphImpl@9e9d28)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.graphx._\n",
    "\n",
    "// Compute Pagerank algorithm\n",
    "val pr = graphXGraph.pageRank(tol = 1.0e-5, resetProb = 0.85).vertices\n",
    "val computedGraphx = (\"graph id\", graphXGraph.joinVertices(pr)(\n",
    "    (id: VertexId, default: Double, pr: Double) => pr)\n",
    ")\n",
    "\n",
    "// Show graph information\n",
    "println(s\"Course ${computedGraphx}:\\n Node count: ${computedGraphx._2.numVertices} \\n Triangle count ${computedGraphx._2.triangleCount()}.\")\n",
    "\n",
    "// Show PageRank for each node\n",
    "computedGraphx._2.vertices.sortBy(_._2.toDouble, ascending = false).take(10).foreach(\n",
    "    vertex => println(s\"-->Nodo ${vertex._1} -> Pagerank: ${vertex._2}\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusions:\n",
    "\n",
    "* GraphX provides few algorithms for graph handling.\n",
    "* The graph building with GraphX is trivial.\n",
    "* GraphX facilitates the use of Spark's RDD.\n",
    "* The Datastax's driver enables Cassandra integration with Spark.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.11",
   "language": "scala211",
   "name": "scala211"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "pygments_lexer": "scala",
   "version": "2.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
