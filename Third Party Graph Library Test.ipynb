{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating graph metrics with third party library\n",
    "\n",
    "The purpouse of this notebook is to explore a third party library called [Graph-Stream](http://graphstream-project.org) for graph handling.\n",
    "\n",
    "Test main features:\n",
    "1. The test was implemented in [Scala 2.11](https://www.scala-lang.org/download/2.11.12.html)\n",
    "2. The processing is handled by [Spark](https://github.com/apache/spark) a cluster computing framework.\n",
    "3. The libraries required to run the test are the following:\n",
    "    * [Cassandra connector](https://github.com/datastax/spark-cassandra-connector) (2.11 scala build version).\n",
    "    * [Graph-stream](http://graphstream-project.org) (Java based library).\n",
    "    * [Spark](https://mvnrepository.com/artifact/org.apache.spark/spark-core_2.11/2.2.1) (2.11 scala build version).\n",
    "4. The Spark version used correspond to standalone application which mean that the use of multiple hosts ecosystem to test isn't needed.\n",
    "\n",
    "## Step 1: Load libraries from Maven\n",
    "\n",
    "In order to download the required libraries from the Maven repositories we need to use the following instructions (special Jupyter notebook's commands that allow Maven integration). In a traditional development environment we use a POM (Maven) or build.sbt (SBT) file to define the dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 114 artifact(s)\n",
      "Adding 13 artifact(s)\n",
      "Adding 4 artifact(s)\n",
      "Adding 5 artifact(s)\n",
      "Adding 7 artifact(s)\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "//Import dependencies from Maven\n",
    "classpath.add(\"org.apache.spark\" % \"spark-core_2.11\" % \"2.1.1\")\n",
    "classpath.add(\"org.apache.spark\" % \"spark-sql_2.11\" % \"2.1.1\")\n",
    "classpath.add(\"com.datastax.spark\" % \"spark-cassandra-connector_2.11\" % \"2.0.0-M3\")\n",
    "classpath.add(\"org.graphstream\" % \"gs-core\" % \"1.3\")\n",
    "classpath.add(\"org.graphstream\" % \"gs-algo\" % \"1.3\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create a Spark context\n",
    "\n",
    "Before start any computation we need to create a Spark context. \n",
    "\n",
    "The following code set up Spark's configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36morg.apache.spark.SparkConf\u001b[0m\n",
       "\u001b[36mconfiguration\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mapache\u001b[0m.\u001b[32mspark\u001b[0m.\u001b[32mSparkConf\u001b[0m = org.apache.spark.SparkConf@8523ff"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.SparkConf\n",
    "val configuration = new SparkConf()\n",
    "    .setAppName(\"Graph-Stream test\")\n",
    "    .setMaster(\"local[*]\") // Use all available computer's cpu cores\n",
    "    .set(\"spark.executor.memory\", \"1g\") // Amount of memory to use per executor process\n",
    "    .set(\"spark.testing.memory\", \"2147480000\")// Avoid any memory issues\n",
    "    .set(\"spark.cassandra.connection.host\", \"127.0.0.1\") // Defines the Cassandra host\n",
    "    .set(\"spark.cassandra.auth.username\", \"cassandra\") //Cassandra credentials\n",
    "    .set(\"spark.cassandra.auth.password\", \"cassandra\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once configured, the Spark context can be created.\n",
    "\n",
    "The following code initialize the Spark context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "18/11/13 08:31:23 INFO SparkContext: Running Spark version 2.1.1\n",
      "18/11/13 08:31:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "18/11/13 08:31:35 WARN Utils: Your hostname, spark resolves to a loopback address: 127.0.1.1; using 192.168.1.207 instead (on interface eth0)\n",
      "18/11/13 08:31:35 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "18/11/13 08:31:36 INFO SecurityManager: Changing view acls to: jose\n",
      "18/11/13 08:31:36 INFO SecurityManager: Changing modify acls to: jose\n",
      "18/11/13 08:31:36 INFO SecurityManager: Changing view acls groups to: \n",
      "18/11/13 08:31:36 INFO SecurityManager: Changing modify acls groups to: \n",
      "18/11/13 08:31:37 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(jose); groups with view permissions: Set(); users  with modify permissions: Set(jose); groups with modify permissions: Set()\n",
      "18/11/13 08:31:51 INFO Utils: Successfully started service 'sparkDriver' on port 43576.\n",
      "18/11/13 08:31:52 INFO SparkEnv: Registering MapOutputTracker\n",
      "18/11/13 08:31:52 INFO SparkEnv: Registering BlockManagerMaster\n",
      "18/11/13 08:31:53 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "18/11/13 08:31:53 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "18/11/13 08:31:54 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-d57f390a-57d2-4be8-b3a2-68022f7f79c0\n",
      "18/11/13 08:31:54 INFO MemoryStore: MemoryStore started with capacity 1048.8 MB\n",
      "18/11/13 08:31:57 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "18/11/13 08:32:06 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "18/11/13 08:32:07 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
      "18/11/13 08:32:07 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.207:4041\n",
      "18/11/13 08:32:10 INFO Executor: Starting executor ID driver on host localhost\n",
      "18/11/13 08:32:11 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45472.\n",
      "18/11/13 08:32:11 INFO NettyBlockTransferService: Server created on 192.168.1.207:45472\n",
      "18/11/13 08:32:12 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "18/11/13 08:32:12 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.207, 45472, None)\n",
      "18/11/13 08:32:12 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.207:45472 with 1048.8 MB RAM, BlockManagerId(driver, 192.168.1.207, 45472, None)\n",
      "18/11/13 08:32:13 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.207, 45472, None)\n",
      "18/11/13 08:32:14 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.1.207, 45472, None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36morg.apache.spark.{SparkContext}\u001b[0m\n",
       "\u001b[36msc\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mapache\u001b[0m.\u001b[32mspark\u001b[0m.\u001b[32mSparkContext\u001b[0m = org.apache.spark.SparkContext@1e15baa"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.{SparkContext}\n",
    "val sc = new SparkContext(configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Retrieve interactions from Cassandra\n",
    "\n",
    "To populate the graph we retrieve the course interactions from Cassandra.\n",
    "\n",
    "First we specify interaction's datatype:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass \u001b[36minteraccion\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "//Interaccion datatype\n",
    "case class interaccion(idinteraccion: String, atributos: Map[String, String]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next lines of code read interactions from Cassandra and save into RDD (Resilient Distributed Dataset):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36mcom.datastax.spark.connector._\u001b[0m\n",
       "\u001b[36minteractions\u001b[0m: \u001b[32mArray\u001b[0m[\u001b[32minteraccion\u001b[0m] = \u001b[33mArray\u001b[0m(\n",
       "  \u001b[33minteraccion\u001b[0m(\n",
       "    \u001b[32m\"1d004db3-fa60-41cd-9ca5-6fca4a7acb49\"\u001b[0m,\n",
       "    \u001b[33mMap\u001b[0m(\n",
       "      \u001b[32m\"tipo_interaccion\"\u001b[0m -> \u001b[32m\"rea\"\u001b[0m,\n",
       "      \u001b[32m\"timestamp\"\u001b[0m -> \u001b[32m\"2007-10-24 08:20:07.263\"\u001b[0m,\n",
       "      \u001b[32m\"tipo_contenido\"\u001b[0m -> \u001b[32m\"des\"\u001b[0m,\n",
       "      \u001b[32m\"id_curso_origen\"\u001b[0m -> \u001b[32m\"1\"\u001b[0m,\n",
       "      \u001b[32m\"nodo_destino\"\u001b[0m -> \u001b[32m\"3\"\u001b[0m,\n",
       "      \u001b[32m\"contenido\"\u001b[0m -> \u001b[32m\"THANKFUL\"\u001b[0m,\n",
       "      \u001b[32m\"plataforma\"\u001b[0m -> \u001b[32m\"f\"\u001b[0m,\n",
       "      \u001b[32m\"id_curso_destino\"\u001b[0m -> \u001b[32m\"1\"\u001b[0m,\n",
       "      \u001b[32m\"sentimiento\"\u001b[0m -> \u001b[32m\"\"\u001b[0m,\n",
       "      \u001b[32m\"nodo_origen\"\u001b[0m -> \u001b[32m\"24\"\u001b[0m\n",
       "    )\n",
       "  ),\n",
       "  \u001b[33minteraccion\u001b[0m(\n",
       "    \u001b[32m\"cc6ea94b-d560-4fae-8549-c12559c91e38\"\u001b[0m,\n",
       "    \u001b[33mMap\u001b[0m(\n",
       "      \u001b[32m\"tipo_interaccion\"\u001b[0m -> \u001b[32m\"men\"\u001b[0m,\n",
       "\u001b[33m...\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import com.datastax.spark.connector._ //Loads implicit functions\n",
    "val interactions = sc.cassandraTable[interaccion](\"diia\", \"interacciones\") // Specify the Cassandra's keyspace and table\n",
    "    .where(\"atributos['id_curso_origen']=?\", 1) // Filter by id_curso = 1\n",
    "    .collect() // Load the data into an RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Build a graph from interactions\n",
    "\n",
    "Graph-Stream offers mainly two ways of graph modeling:\n",
    "* Single-graph: Is a graph that supports only one edge betweeen two nodes.\n",
    "* Multi-graph: A graph that can have multiple edges between two nodes.\n",
    "\n",
    "In this case we use a multi-graph because we need to differentiate in-degree and out-degree. Another aspect that we can take advantage is Graph-stream's features that permit graph auto creation only from the interactions set\n",
    "\n",
    "The next code iterate over each interaction previously fetched to fill the Multi-graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Nodes->46\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36morg.graphstream.graph.implementations.MultiGraph\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.graphstream.graph.{Edge, Node}\u001b[0m\n",
       "\u001b[36mgraph\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mgraphstream\u001b[0m.\u001b[32mgraph\u001b[0m.\u001b[32mimplementations\u001b[0m.\u001b[32mMultiGraph\u001b[0m = DIIA Graph"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.graphstream.graph.implementations.MultiGraph\n",
    "import org.graphstream.graph.{Edge, Node}\n",
    "\n",
    "val graph: MultiGraph = new MultiGraph(\"DIIA Graph\")\n",
    "\n",
    "// Enable graph auto-creation only from interactions\n",
    "graph.setStrict(false)\n",
    "graph.setAutoCreate(true)\n",
    "\n",
    "interactions.foreach(\n",
    "    interaction => graph.addEdge[Edge](interaction.idinteraccion.toString, interaction.atributos.get(\"nodo_origen\").toString, interaction.atributos.get(\"nodo_destino\").toString)\n",
    ")\n",
    "System.out.println(\"Number of Nodes->\" + graph.getNodeCount + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once modeled, we can get the degree of each node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nodeId->Some(24)\n",
      "inDegree->11\n",
      "outDegree->11\n",
      "\n",
      "nodeId->Some(3)\n",
      "inDegree->61\n",
      "outDegree->61\n",
      "\n",
      "nodeId->Some(19)\n",
      "inDegree->81\n",
      "outDegree->81\n",
      "\n",
      "nodeId->Some(17)\n",
      "inDegree->34\n",
      "outDegree->34\n",
      "\n",
      "nodeId->Some()\n",
      "inDegree->346\n",
      "outDegree->346\n",
      "\n",
      "nodeId->Some(13)\n",
      "inDegree->11\n",
      "outDegree->11\n",
      "\n",
      "nodeId->Some(29)\n",
      "inDegree->3\n",
      "outDegree->3\n",
      "\n",
      "nodeId->Some(7)\n",
      "inDegree->33\n",
      "outDegree->33\n",
      "\n",
      "nodeId->Some(14)\n",
      "inDegree->38\n",
      "outDegree->38\n",
      "\n",
      "nodeId->Some(18)\n",
      "inDegree->34\n",
      "outDegree->34\n",
      "\n",
      "nodeId->Some(4)\n",
      "inDegree->39\n",
      "outDegree->39\n",
      "\n",
      "nodeId->Some(10)\n",
      "inDegree->76\n",
      "outDegree->76\n",
      "\n",
      "nodeId->Some(5)\n",
      "inDegree->40\n",
      "outDegree->40\n",
      "\n",
      "nodeId->Some(25)\n",
      "inDegree->111\n",
      "outDegree->111\n",
      "\n",
      "nodeId->Some(16)\n",
      "inDegree->49\n",
      "outDegree->49\n",
      "\n",
      "nodeId->Some(20)\n",
      "inDegree->26\n",
      "outDegree->26\n",
      "\n",
      "nodeId->Some(21)\n",
      "inDegree->28\n",
      "outDegree->28\n",
      "\n",
      "nodeId->Some(0)\n",
      "inDegree->47\n",
      "outDegree->47\n",
      "\n",
      "nodeId->Some(22)\n",
      "inDegree->44\n",
      "outDegree->44\n",
      "\n",
      "nodeId->Some(1)\n",
      "inDegree->46\n",
      "outDegree->46\n",
      "\n",
      "nodeId->Some(9)\n",
      "inDegree->16\n",
      "outDegree->16\n",
      "\n",
      "nodeId->Some(23)\n",
      "inDegree->13\n",
      "outDegree->13\n",
      "\n",
      "nodeId->Some(11)\n",
      "inDegree->17\n",
      "outDegree->17\n",
      "\n",
      "nodeId->Some(34)\n",
      "inDegree->2\n",
      "outDegree->2\n",
      "\n",
      "nodeId->Some(46)\n",
      "inDegree->6\n",
      "outDegree->6\n",
      "\n",
      "nodeId->Some(26)\n",
      "inDegree->6\n",
      "outDegree->6\n",
      "\n",
      "nodeId->Some(2)\n",
      "inDegree->52\n",
      "outDegree->52\n",
      "\n",
      "nodeId->Some(32)\n",
      "inDegree->1\n",
      "outDegree->1\n",
      "\n",
      "nodeId->Some(15)\n",
      "inDegree->38\n",
      "outDegree->38\n",
      "\n",
      "nodeId->Some(33)\n",
      "inDegree->5\n",
      "outDegree->5\n",
      "\n",
      "nodeId->Some(27)\n",
      "inDegree->6\n",
      "outDegree->6\n",
      "\n",
      "nodeId->Some(44)\n",
      "inDegree->2\n",
      "outDegree->2\n",
      "\n",
      "nodeId->Some(37)\n",
      "inDegree->5\n",
      "outDegree->5\n",
      "\n",
      "nodeId->Some(30)\n",
      "inDegree->4\n",
      "outDegree->4\n",
      "\n",
      "nodeId->Some(50)\n",
      "inDegree->5\n",
      "outDegree->5\n",
      "\n",
      "nodeId->Some(6)\n",
      "inDegree->31\n",
      "outDegree->31\n",
      "\n",
      "nodeId->Some(35)\n",
      "inDegree->2\n",
      "outDegree->2\n",
      "\n",
      "nodeId->Some(28)\n",
      "inDegree->3\n",
      "outDegree->3\n",
      "\n",
      "nodeId->Some(42)\n",
      "inDegree->4\n",
      "outDegree->4\n",
      "\n",
      "nodeId->Some(38)\n",
      "inDegree->2\n",
      "outDegree->2\n",
      "\n",
      "nodeId->Some(45)\n",
      "inDegree->5\n",
      "outDegree->5\n",
      "\n",
      "nodeId->Some(36)\n",
      "inDegree->4\n",
      "outDegree->4\n",
      "\n",
      "nodeId->Some(43)\n",
      "inDegree->3\n",
      "outDegree->3\n",
      "\n",
      "nodeId->Some(40)\n",
      "inDegree->4\n",
      "outDegree->4\n",
      "\n",
      "nodeId->Some(31)\n",
      "inDegree->1\n",
      "outDegree->1\n",
      "\n",
      "nodeId->Some(12)\n",
      "inDegree->3\n",
      "outDegree->3\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36mscala.collection.JavaConverters._\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import scala.collection.JavaConverters._\n",
    "graph.getNodeSet[Node].asScala.toArray.foreach( // Iterate over the graph's nodes\n",
    "    // Show node's degree\n",
    "    (n: Node) => System.out.println(\"nodeId->\" + n.getId + \"\\n\" + \"inDegree->\" + n.getInDegree + \"\\n\" + \"outDegree->\" + n.getOutDegree + \"\\n\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Calculate the mainly centrality metrics\n",
    "\n",
    "In this section we will calculate the main centrality metrics with the third party's build-in algorithms.\n",
    "\n",
    "### Betweenness centrality\n",
    "The betweenness centrality counts how many shortest paths between each pair of nodes of the graph pass by a node. It does it for all nodes of the graph. This measure might identify nodes with the ability to control information flow between different parts of the network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Betweenness:\n",
      "Some(24)->3.948859167811559\n",
      "Some(3)->214.91816770214086\n",
      "Some(19)->137.94334137917892\n",
      "Some(17)->5.843250215552445\n",
      "Some()->54.91488195864106\n",
      "Some(13)->7.296179775925958\n",
      "Some(29)->0.13008054522021847\n",
      "Some(7)->25.486547416762484\n",
      "Some(14)->41.28244434478991\n",
      "Some(18)->45.999224049807836\n",
      "Some(4)->46.0981327374728\n",
      "Some(10)->87.98611020325387\n",
      "Some(5)->16.494914794409716\n",
      "Some(25)->9.13192175203748\n",
      "Some(16)->70.73558697629242\n",
      "Some(20)->1.6831236682655226\n",
      "Some(21)->0.0\n",
      "Some(0)->65.25229275949636\n",
      "Some(22)->58.76973893881553\n",
      "Some(1)->116.96977446394595\n",
      "Some(9)->21.28107430317513\n",
      "Some(23)->0.0\n",
      "Some(11)->16.511937054767813\n",
      "Some(34)->0.010221258539087452\n",
      "Some(46)->0.02182085065551521\n",
      "Some(26)->0.058103340942104176\n",
      "Some(2)->74.83012409067176\n",
      "Some(32)->0.0\n",
      "Some(15)->87.1495471513281\n",
      "Some(33)->0.10294117647058823\n",
      "Some(27)->0.6815622105920432\n",
      "Some(44)->0.0\n",
      "Some(37)->0.07774576763340382\n",
      "Some(30)->0.04793403066466102\n",
      "Some(50)->0.022240364043704844\n",
      "Some(6)->10.96686764237303\n",
      "Some(35)->0.0\n",
      "Some(28)->0.0841581734257455\n",
      "Some(42)->0.010672016607106783\n",
      "Some(38)->0.0\n",
      "Some(45)->0.07334491410924532\n",
      "Some(36)->0.011930161439442429\n",
      "Some(43)->0.0\n",
      "Some(40)->0.1664281552592808\n",
      "Some(31)->0.0\n",
      "Some(12)->0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36morg.graphstream.algorithm.BetweennessCentrality\u001b[0m\n",
       "\u001b[36mbcb\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mgraphstream\u001b[0m.\u001b[32malgorithm\u001b[0m.\u001b[32mBetweennessCentrality\u001b[0m = org.graphstream.algorithm.BetweennessCentrality@c5b73b"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.graphstream.algorithm.BetweennessCentrality\n",
    "\n",
    "// Compute the betweenness centrality for each node in the graph.\n",
    "val bcb = new BetweennessCentrality\n",
    "bcb.init(graph)\n",
    "bcb.compute()\n",
    "\n",
    "System.out.println(\"Betweenness:\")\n",
    "// Iterate over each graph's node and show it calculated value\n",
    "graph.getNodeSet[Node].asScala.toArray.foreach(\n",
    "    (n: Node) => System.out.println(n.getId + \"->\" + n.getAttribute(\"Cb\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pagerank algorithm\n",
    "\n",
    "The PageRank algorithm measures the \"importance\" of the nodes in a graph. It assigns to each node a rank. This rank corresponds to the probability that a \"random surfer\" visits the node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pagerank:\n",
      "Some(24)->0.010655649482737698\n",
      "Some(3)->0.04694391120351182\n",
      "Some(19)->0.05274289911538274\n",
      "Some(17)->0.021791345457441536\n",
      "Some()->0.20429061178450317\n",
      "Some(13)->0.010407576841936775\n",
      "Some(29)->0.005331746474090557\n",
      "Some(7)->0.02464746772098131\n",
      "Some(14)->0.02752087671217578\n",
      "Some(18)->0.026085763592871097\n",
      "Some(4)->0.029153790096185166\n",
      "Some(10)->0.0481720583178849\n",
      "Some(5)->0.025843894235493343\n",
      "Some(25)->0.06645453914319224\n",
      "Some(16)->0.032814932361962285\n",
      "Some(20)->0.017211742323640006\n",
      "Some(21)->0.01731296161404435\n",
      "Some(0)->0.03311769577390839\n",
      "Some(22)->0.03140799843118387\n",
      "Some(1)->0.03416099735127152\n",
      "Some(9)->0.014236612081643183\n",
      "Some(23)->0.009785055159315627\n",
      "Some(11)->0.01361290912561936\n",
      "Some(34)->0.0045107663185433\n",
      "Some(46)->0.006960687207748007\n",
      "Some(26)->0.007050894004983111\n",
      "Some(2)->0.035185060879378116\n",
      "Some(32)->0.003836013511585187\n",
      "Some(15)->0.028899268160889096\n",
      "Some(33)->0.006544963372927964\n",
      "Some(27)->0.007383734688100394\n",
      "Some(44)->0.004434497368622707\n",
      "Some(37)->0.0063045665918122305\n",
      "Some(30)->0.005605973083823735\n",
      "Some(50)->0.006259017576871671\n",
      "Some(6)->0.021382640688358368\n",
      "Some(35)->0.004468488844252735\n",
      "Some(28)->0.0052205217150898525\n",
      "Some(42)->0.005595275181178914\n",
      "Some(38)->0.004458747104914734\n",
      "Some(45)->0.006361490269164679\n",
      "Some(36)->0.005741337595697754\n",
      "Some(43)->0.00515413256249297\n",
      "Some(40)->0.005857679709359715\n",
      "Some(31)->0.003915010219120085\n",
      "Some(12)->0.005166198944108219\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36morg.graphstream.algorithm.PageRank\u001b[0m\n",
       "\u001b[36mpageRank\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mgraphstream\u001b[0m.\u001b[32malgorithm\u001b[0m.\u001b[32mPageRank\u001b[0m = org.graphstream.algorithm.PageRank@1c31c60"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.graphstream.algorithm.PageRank\n",
    "\n",
    "// Initialization of the algorithm \n",
    "val pageRank = new PageRank\n",
    "pageRank.init(graph)\n",
    "pageRank.compute()\n",
    "\n",
    "System.out.println(\"Pagerank:\")\n",
    "graph.getNodeSet[Node].asScala.toArray.foreach( // Iterave over each graph's node and print the calculated node's pagerank value\n",
    "    (n: Node) => System.out.println(n.getId + \"->\" + n.getAttribute(\"PageRank\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eccentricity algorithm\n",
    "\n",
    "The Eccentricity (node's largest geodesic) measure how far an node is from the furthest other.\n",
    "\n",
    "This algorithm needs that APSP (All Pair Shortest Path) algorithm has been computed before its own computation. The following code calculate the shortest path from any node to any destination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36morg.graphstream.algorithm.APSP\u001b[0m\n",
       "\u001b[36mapsp\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mgraphstream\u001b[0m.\u001b[32malgorithm\u001b[0m.\u001b[32mAPSP\u001b[0m = org.graphstream.algorithm.APSP@7c9cd4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.graphstream.algorithm.APSP\n",
    "val apsp = new APSP\n",
    "apsp.init(graph)\n",
    "apsp.setDirected(true)\n",
    "apsp.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the eccentricity algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eccentricity:\n",
      "Some(24)->false\n",
      "Some(3)->false\n",
      "Some(19)->true\n",
      "Some(17)->false\n",
      "Some()->true\n",
      "Some(13)->false\n",
      "Some(29)->false\n",
      "Some(7)->false\n",
      "Some(14)->true\n",
      "Some(18)->false\n",
      "Some(4)->false\n",
      "Some(10)->true\n",
      "Some(5)->false\n",
      "Some(25)->true\n",
      "Some(16)->false\n",
      "Some(20)->false\n",
      "Some(21)->false\n",
      "Some(0)->true\n",
      "Some(22)->false\n",
      "Some(1)->true\n",
      "Some(9)->false\n",
      "Some(23)->false\n",
      "Some(11)->false\n",
      "Some(34)->false\n",
      "Some(46)->false\n",
      "Some(26)->false\n",
      "Some(2)->false\n",
      "Some(32)->false\n",
      "Some(15)->true\n",
      "Some(33)->false\n",
      "Some(27)->false\n",
      "Some(44)->false\n",
      "Some(37)->false\n",
      "Some(30)->false\n",
      "Some(50)->false\n",
      "Some(6)->false\n",
      "Some(35)->false\n",
      "Some(28)->false\n",
      "Some(42)->false\n",
      "Some(38)->false\n",
      "Some(45)->false\n",
      "Some(36)->false\n",
      "Some(43)->false\n",
      "Some(40)->false\n",
      "Some(31)->false\n",
      "Some(12)->false\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36morg.graphstream.algorithm.Eccentricity\u001b[0m\n",
       "\u001b[36meccentricity\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mgraphstream\u001b[0m.\u001b[32malgorithm\u001b[0m.\u001b[32mEccentricity\u001b[0m = org.graphstream.algorithm.Eccentricity@a1e145"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.graphstream.algorithm.Eccentricity\n",
    "\n",
    "val eccentricity = new Eccentricity\n",
    "eccentricity.init(graph)\n",
    "eccentricity.compute()\n",
    "System.out.println(\"Eccentricity:\")\n",
    "graph.getNodeSet[Node].asScala.toArray.foreach(\n",
    "    (n: Node) => System.out.println(n.getId + \"->\" + n.getAttribute(\"eccentricity\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Closeness centrality measures the proximity of an node to the other nodes in the social network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closenness:\n",
      "Some(24)->0.011764705882352941\n",
      "Some(3)->0.015151515151515152\n",
      "Some(19)->0.014925373134328358\n",
      "Some(17)->0.011494252873563218\n",
      "Some()->0.015151515151515152\n",
      "Some(13)->0.009708737864077669\n",
      "Some(29)->0.009708737864077669\n",
      "Some(7)->0.012048192771084338\n",
      "Some(14)->0.0136986301369863\n",
      "Some(18)->0.01282051282051282\n",
      "Some(4)->0.012987012987012988\n",
      "Some(10)->0.014084507042253521\n",
      "Some(5)->0.011904761904761904\n",
      "Some(25)->0.014492753623188406\n",
      "Some(16)->0.013333333333333334\n",
      "Some(20)->0.010869565217391304\n",
      "Some(21)->0.00909090909090909\n",
      "Some(0)->0.0136986301369863\n",
      "Some(22)->0.013888888888888888\n",
      "Some(1)->0.014925373134328358\n",
      "Some(9)->0.011627906976744186\n",
      "Some(23)->0.00909090909090909\n",
      "Some(11)->0.011235955056179775\n",
      "Some(34)->0.008695652173913044\n",
      "Some(46)->0.011111111111111112\n",
      "Some(26)->0.01098901098901099\n",
      "Some(2)->0.013157894736842105\n",
      "Some(32)->0.008333333333333333\n",
      "Some(15)->0.014705882352941176\n",
      "Some(33)->0.010416666666666666\n",
      "Some(27)->0.010526315789473684\n",
      "Some(44)->0.00909090909090909\n",
      "Some(37)->0.010638297872340425\n",
      "Some(30)->0.010101010101010102\n",
      "Some(50)->0.010638297872340425\n",
      "Some(6)->0.013333333333333334\n",
      "Some(35)->0.01\n",
      "Some(28)->0.00980392156862745\n",
      "Some(42)->0.010416666666666666\n",
      "Some(38)->0.008547008547008548\n",
      "Some(45)->0.010526315789473684\n",
      "Some(36)->0.010309278350515464\n",
      "Some(43)->0.009615384615384616\n",
      "Some(40)->0.01020408163265306\n",
      "Some(31)->0.00909090909090909\n",
      "Some(12)->0.00980392156862745\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36morg.graphstream.algorithm.measure.ClosenessCentrality\u001b[0m\n",
       "\u001b[36mcloseness\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mgraphstream\u001b[0m.\u001b[32malgorithm\u001b[0m.\u001b[32mmeasure\u001b[0m.\u001b[32mClosenessCentrality\u001b[0m = org.graphstream.algorithm.measure.ClosenessCentrality@11f11fe"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// Calculate and show node's closeness\n",
    "import org.graphstream.algorithm.measure.ClosenessCentrality\n",
    "val closeness: ClosenessCentrality = new ClosenessCentrality ()\n",
    "closeness.init(graph)\n",
    "closeness.compute()\n",
    "System.out.println(\"Closenness:\")\n",
    "graph.getNodeSet[Node].asScala.toArray.foreach(\n",
    "    (n: Node) => System.out.println(n.getId + \"->\" + n.getAttribute(\"closeness\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm implementation\n",
    "\n",
    "In order to take advantage of Graph-Stream's features like APSP, we will implement our own version of closeness metric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Custom closenness implementation:\n",
      "Some(24)->0.011764705882352941\n",
      "Some(3)->0.015151515151515152\n",
      "Some(19)->0.014925373134328358\n",
      "Some(17)->0.011494252873563218\n",
      "Some()->0.015151515151515152\n",
      "Some(13)->0.009708737864077669\n",
      "Some(29)->0.009708737864077669\n",
      "Some(7)->0.012048192771084338\n",
      "Some(14)->0.0136986301369863\n",
      "Some(18)->0.01282051282051282\n",
      "Some(4)->0.012987012987012988\n",
      "Some(10)->0.014084507042253521\n",
      "Some(5)->0.011904761904761904\n",
      "Some(25)->0.014492753623188406\n",
      "Some(16)->0.013333333333333334\n",
      "Some(20)->0.010869565217391304\n",
      "Some(21)->0.00909090909090909\n",
      "Some(0)->0.0136986301369863\n",
      "Some(22)->0.013888888888888888\n",
      "Some(1)->0.014925373134328358\n",
      "Some(9)->0.011627906976744186\n",
      "Some(23)->0.00909090909090909\n",
      "Some(11)->0.011235955056179775\n",
      "Some(34)->0.008695652173913044\n",
      "Some(46)->0.011111111111111112\n",
      "Some(26)->0.01098901098901099\n",
      "Some(2)->0.013157894736842105\n",
      "Some(32)->0.008333333333333333\n",
      "Some(15)->0.014705882352941176\n",
      "Some(33)->0.010416666666666666\n",
      "Some(27)->0.010526315789473684\n",
      "Some(44)->0.00909090909090909\n",
      "Some(37)->0.010638297872340425\n",
      "Some(30)->0.010101010101010102\n",
      "Some(50)->0.010638297872340425\n",
      "Some(6)->0.013333333333333334\n",
      "Some(35)->0.01\n",
      "Some(28)->0.00980392156862745\n",
      "Some(42)->0.010416666666666666\n",
      "Some(38)->0.008547008547008548\n",
      "Some(45)->0.010526315789473684\n",
      "Some(36)->0.010309278350515464\n",
      "Some(43)->0.009615384615384616\n",
      "Some(40)->0.01020408163265306\n",
      "Some(31)->0.00909090909090909\n",
      "Some(12)->0.00980392156862745\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36morg.graphstream.algorithm.APSP.APSPInfo\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "System.out.println(\"\\nCustom closenness implementation:\")\n",
    "import org.graphstream.algorithm.APSP.APSPInfo\n",
    "\n",
    "graph.getNodeSet[Node].asScala.toArray.foreach(\n",
    "    (x: Node) => {\n",
    "      var sum = 0.0\n",
    "\n",
    "      graph.getNodeSet[Node].asScala.toArray.foreach(\n",
    "        (y: Node) => {\n",
    "          if (!x.getId.equals(y.getId)) {\n",
    "            val info:APSPInfo = x.getAttribute[APSPInfo](APSP.APSPInfo.ATTRIBUTE_NAME)\n",
    "            sum += info.getShortestPathTo(y.getId).getEdgeCount\n",
    "          }\n",
    "        }\n",
    "      )\n",
    "\n",
    "      val closenness = (1 / sum)\n",
    "      System.out.println(x.getId+ \"->\" + closenness)\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// free spark's resources\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions:\n",
    "\n",
    "* Graph-stream can interoperate with the Scala language and the Spark framework without problems.\n",
    "* The third party offers a wide variety of build-in algorithms.\n",
    "* It brings the possibility to implement additional algorithms that take advantage of features like APSP.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.11",
   "language": "scala211",
   "name": "scala211"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "pygments_lexer": "scala",
   "version": "2.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
